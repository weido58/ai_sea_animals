#  大模型理解上下文



<img src="image/24.png">

## 大模型理解上下文（多轮对话）
<img src="image/25.png">

 好了继续我们上面的，从图中可以看出

⼤模型并不能够理解” 再说⼀次 “ 的含义。这是因为，在⽬前的代码中，每次调⽤就是⼀次新的会话。
    

所以对⼤模型来说，没有第⼀次的“ 你是谁 ” 的记录，⼤模型⾃然就⽆法理解 “ 再说⼀次 ” 的含义。
如何让⼤模型能够了解之前的聊天记录呢？

代码如下：



但是如果要我们每次把之前的记录自己去维护， 未免太麻烦， 所以提供了ChatMemory。

实现方式,通过加入一个配置类来实现：

~~~ java
package com.gec.marine.config;

import dev.langchain4j.memory.ChatMemory;
import dev.langchain4j.memory.chat.MessageWindowChatMemory;
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.chat.StreamingChatLanguageModel;
import dev.langchain4j.model.openai.OpenAiChatModel;
import dev.langchain4j.model.openai.OpenAiStreamingChatModel;
import dev.langchain4j.service.AiServices;
import dev.langchain4j.service.TokenStream;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class AssistantConfig {

    // 从配置文件中注入OpenAI API密钥
    @Value("${langchain4j.open-ai.chat-model.api-key}")
    private String apiKey;

    // 从配置文件中注入OpenAI API基础URL
    @Value("${langchain4j.open-ai.chat-model.base-url}")
    private String baseUrl;

    // 从配置文件中注入使用的模型名称
    @Value("${langchain4j.open-ai.chat-model.model-name}")
    private String modelName;

    // 定义AI助手的公共接口
    public interface Assistant{
        // 普通聊天方法（阻塞式，返回完整响应）
        String chat(String message);

        // 流式响应方法（实时返回生成的token）
        TokenStream stream(String message);
    }


    /**
     * 配置常规的聊天语言模型（阻塞式）
     *
     * @return ChatLanguageModel实例
     */
    @Bean
    public ChatLanguageModel chatLanguageModel() {
        return OpenAiChatModel.builder()
                .apiKey(apiKey)
                .baseUrl(baseUrl)
                .modelName(modelName)
                .build();
    }

    /**
     * 配置流式聊天语言模型（非阻塞式）
     * 支持实时流式输出，适合长对话场景
     *
     * @return StreamingChatLanguageModel实例
     */
    @Bean
    public StreamingChatLanguageModel streamingChatLanguageModel() {
        return OpenAiStreamingChatModel.builder()
                .apiKey(apiKey)
                .baseUrl(baseUrl)
                .modelName(modelName)
                .build();
    }


    // 定义Spring Bean来创建助手实例
    @Bean  // 标记为Spring管理的Bean
    public Assistant getAssistant(
            // 注入标准聊天语言模型（用于普通响应）
            ChatLanguageModel chatLanguageModel,
            // 注入流式聊天语言模型（用于流式响应）
            StreamingChatLanguageModel streamingChatLanguageModel){

        // 创建聊天记忆，保留最近10条消息的对话上下文
        ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

        // 使用AI服务构建助手实现
        Assistant assistant = AiServices.builder(Assistant.class)
                // 设置标准聊天模型（用于chat()方法）
                .chatLanguageModel(chatLanguageModel)
                // 设置流式聊天模型（用于stream()方法）
                .streamingChatLanguageModel(streamingChatLanguageModel)
                // 配置聊天记忆保持对话上下文
                .chatMemory(chatMemory)
                // 完成构建
                .build();

        // 返回配置好的助手实例
        return assistant;
    }
}

~~~



原理：

0. 通过AiService创建的代理对象（AiServices.builder(Assistant.class)）调用chat方法
1. 代理对象会去ChatMemory中获取之前的对话记录（获取记忆）
2. 将获取到的对话记录合并到当前对话中（此时大模型根据之前的聊天记录肯定就拥有了“记忆”）
3. 将当前的对话内容存入ChatMemory（保存记忆）

<img src="image/26.png">

上面的代码还需要配置streamingChatLanguageModel流式聊天对象：

将deepseek的streamingChatLanguageModel配置信息配置到application.yml中

```
langchain4j:
  open-ai:
    chat-model:
      api-key: sk-0c650aa2ed834330b0e153bf42347e20
      base-url: https://api.deepseek.com
      model-name: deepseek-chat
```

编写控制器代码

```
package com.gec.marine.controller;

import com.gec.marine.config.AssistantConfig;
import com.gec.marine.service.ChatService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;

/**
 * 聊天控制器
 * 提供REST API接口，处理HTTP请求
 *
 * @author Your Name
 * @version 1.0
 */
@RestController  // Spring注解：标记为REST控制器，自动序列化返回值为JSON
@RequestMapping("/chat")  // 设置控制器的基础路径
public class ChatController {

    /**
     * 自动注入聊天服务
     * Spring会自动找到ChatService的实例并注入
     */
    @Autowired
    private ChatService chatService;
    @Autowired
    private AssistantConfig.Assistant  assistant;

    /**
     * POST接口：接收用户消息并返回AI回复
     *
     * @param message 请求体中的用户消息
     * @return AI模型的回复
     */
    @GetMapping("/sendmessage")
    public String chat(@RequestParam("message") String message) {
        // 调用服务层方法处理聊天逻辑
        return chatService.chat(message);
    }

    @GetMapping("/sendmessageByAssistant")
    public String sendmessageByAssistant(@RequestParam("message") String message) {
        // 调用服务层方法处理聊天逻辑
        return assistant.chat(message);
    }

    /**
     * GET接口：测试用的hello接口
     * 用于快速验证系统是否正常工作
     *
     * @return AI对"你好，你是谁？"的回复
     */
    @GetMapping("/hello")  // 映射GET请求到/api/chat/hello
    public String hello() {
        // 发送固定的问候消息给AI
        return chatService.chat("你好，你是谁？");
    }
}
```

可以观察到响应回来的字符串是直接一次性显示出来的：

<img src="image/33.jpg">

再次提问看看它是否能够记住我们的姓名：

<img src="image/34.jpg">

对比一下官方的DeepSeek聊天工具，我们可以看出它的聊天信息是一个一个文字显示出来的，那么这种功能是如何实现的呢？

## 流式输出

接下来如果我们需要在自己的控制器中实现聊天信息是一个一个文字显示出来的使用的话，可以引入流式输出的依赖webflux：

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-webflux</artifactId>
</dependency>
```

引入 WebFlux 的核心目的是为了：

1. **高效处理流式 AI 响应**（如逐词生成）。
2. **提升高并发下的系统吞吐量**。
3. **与响应式 AI 组件（如 `StreamingChatLanguageModel`）无缝集成**。

编写控制器代码:

```
package com.gec.marine.controller;

import com.gec.marine.config.AssistantConfig;
import com.gec.marine.service.ChatService;
import dev.langchain4j.service.TokenStream;
import jakarta.servlet.http.HttpServletResponse;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;

/**
 * 聊天控制器
 * 提供REST API接口，处理HTTP请求
 *
 * @author Your Name
 * @version 1.0
 */
@RestController  // Spring注解：标记为REST控制器，自动序列化返回值为JSON
@RequestMapping("/chat")  // 设置控制器的基础路径
public class ChatController {

    /**
     * 自动注入聊天服务
     * Spring会自动找到ChatService的实例并注入
     */
    @Autowired
    private ChatService chatService;
    @Autowired
    private AssistantConfig.Assistant  assistant;

    /**
     * POST接口：接收用户消息并返回AI回复
     *
     * @param message 请求体中的用户消息
     * @return AI模型的回复
     */
    @GetMapping("/sendmessage")
    public String chat(@RequestParam("message") String message) {
        // 调用服务层方法处理聊天逻辑
        return chatService.chat(message);
    }

    @GetMapping("/sendmessageByAssistant")
    public String sendmessageByAssistant(@RequestParam("message") String message) {
        // 调用服务层方法处理聊天逻辑
        return assistant.chat(message);
    }

    /**
     * GET接口：测试用的hello接口
     * 用于快速验证系统是否正常工作
     *
     * @return AI对"你好，你是谁？"的回复
     */
    @GetMapping("/hello")  // 映射GET请求到/api/chat/hello
    public String hello() {
        // 发送固定的问候消息给AI
        return chatService.chat("你好，你是谁？");
    }

    // 定义HTTP接口端点，处理带有记忆功能的流式聊天请求
    @RequestMapping(value = "/memory_steam_chat",produces = "text/stream;charset=utf-8")
// 返回Flux<String>类型实现流式响应，参数message默认值为"我是谁"
    public Flux<String> memoryStreamChat(
            // 从请求参数获取用户消息，默认值"我是谁"
            @RequestParam(defaultValue = "我是谁") String message,
            // 注入HttpServletResponse对象（虽然响应式编程中通常不直接使用）
            HttpServletResponse response) {

        // 调用助手服务的流式接口，获取TokenStream对象
        TokenStream stream = assistant.stream(message);

        // 创建Flux流式响应
        return Flux.create(sink -> {
            // 设置部分响应回调：每次收到部分响应时通过sink发送数据
            stream.onPartialResponse(s -> sink.next(s))
                    // 设置完成回调：当收到完成信号时关闭流
                    .onCompleteResponse(c-> sink.complete())
                    // 设置错误回调：发生错误时传递错误信号 它的作用是将 sink 对象的 error 方法作为函数式接口的实现传递进去。
                    .onError(sink::error)//stream.onError(error -> sink.error(error));
                    // 启动流处理
                    .start();
        });
    }
}
```

输出结果：

<img src="image/27.png">

再说一次我是谁：

<img src="image/28.png">